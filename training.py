# =============================================================================
# TRAINING WITH SPLIT DATA - COMPLETE VERSION WITH BASE vs TRAINED COMPARISON
# =============================================================================

import sys
import os
import json
from datetime import datetime
import hashlib
from google.colab import files

# Setup path
sys.path.append('/content/PRUEBA')

# Import transformers (CORRECTED)
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW  # ‚Üê From torch, not from transformers
import torch

# Load split data (created in setup.py)
from DATA.train import train_data
from DATA.val import val_data
from DATA.test import test_data

print(f"üìä Split data loaded:")
print(f"   ‚Ä¢ Train: {len(train_data)} examples")
print(f"   ‚Ä¢ Val: {len(val_data)} examples")
print(f"   ‚Ä¢ Test: {len(test_data)} examples")

# CONFIGURATION FOR A100 (THIS WAS MISSING)
CONFIG = {
    'model_name': 'Salesforce/codet5-base',
    'batch_size': 6,        # Conservative for small dataset
    'learning_rate': 4e-5,  #5.2
    'epochs': 10, #20
    'max_length': 256,
    'device': 'cuda'
}

print(f"\n‚öôÔ∏è Configuration:")
for key, value in CONFIG.items():
    print(f"   ‚Ä¢ {key}: {value}")

# Load model
print(f"\nü§ñ Loading {CONFIG['model_name']}...")
tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])
model = AutoModelForSeq2SeqLM.from_pretrained(CONFIG['model_name'])
model = model.to(CONFIG['device'])

print(f"‚úÖ Model loaded on GPU")
print(f"‚úÖ Parameters: {model.num_parameters():,}")

def safe_download_model(model_path, model_name="trained_model"):
    """Descarga segura con verificaci√≥n de integridad"""
    
    print(f"\nüíæ INICIANDO DESCARGA SEGURA DE {model_name}")
    print("=" * 60)
    
    try:
        # 1. Verificar que el archivo existe
        if not os.path.exists(model_path):
            print(f"‚ùå Archivo no encontrado: {model_path}")
            return False
        
        # 2. Verificar integridad del modelo
        print(f"üîç Verificando integridad del modelo...")
        state_dict = torch.load(model_path, map_location='cpu')
        file_size = os.path.getsize(model_path)
        
        print(f"‚úÖ Modelo v√°lido:")
        print(f"   üìè Tama√±o: {file_size / (1024*1024):.2f} MB")
        print(f"   üîë Par√°metros: {len(state_dict.keys())} grupos")
        
        # 3. Calcular hash para verificaci√≥n posterior
        print(f"üîí Calculando hash MD5...")
        with open(model_path, 'rb') as f:
            file_hash = hashlib.md5(f.read()).hexdigest()
        print(f"   Hash MD5: {file_hash}")
        
        # 4. Crear archivo de verificaci√≥n
        hash_file = model_path.replace('.pth', '_hash.txt')
        with open(hash_file, 'w') as f:
            f.write(f"Model: {os.path.basename(model_path)}\n")
            f.write(f"Size: {file_size} bytes\n")
            f.write(f"MD5: {file_hash}\n")
            f.write(f"Parameters: {len(state_dict.keys())} groups\n")
        
        print(f"üìÑ Archivo de verificaci√≥n creado: {hash_file}")
        
        # 5. Descargar ambos archivos
        print(f"üì• Descargando modelo...")
        files.download(model_path)
        
        print(f"üì• Descargando archivo de verificaci√≥n...")
        files.download(hash_file)
        
        print(f"‚úÖ DESCARGA COMPLETADA EXITOSAMENTE")
        print(f"üí° Usa el archivo {os.path.basename(hash_file)} para verificar la integridad")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error durante la descarga: {e}")
        return False

# Dataset class
class RoboticsDataset(Dataset):
    def __init__(self, data, tokenizer, max_length=256):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        
        # Tokenize input
        input_encoding = self.tokenizer(
            item['input'],
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        # Tokenize target
        target_encoding = self.tokenizer(
            item['output'],
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': input_encoding['input_ids'].flatten(),
            'attention_mask': input_encoding['attention_mask'].flatten(),
            'labels': target_encoding['input_ids'].flatten()
        }

# Create datasets
train_dataset = RoboticsDataset(train_data, tokenizer, CONFIG['max_length'])
val_dataset = RoboticsDataset(val_data, tokenizer, CONFIG['max_length'])
test_dataset = RoboticsDataset(test_data, tokenizer, CONFIG['max_length'])

# Create dataloaders
train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False)

# Optimizer
optimizer = AdamW(model.parameters(), lr=CONFIG['learning_rate'])

print("‚úÖ Datasets and optimizer configured")

# =============================================================================
# FUNCTION TO TEST MODEL PERFORMANCE
# =============================================================================
def test_model_performance(model, test_data, tokenizer, device, config, model_name="Model"):
    """Test model and return detailed results"""
    model.eval()
    
    print(f"\nüîç TESTING {model_name}")
    print("=" * 60)
    
    test_examples = [item['input'] for item in test_data]
    perfect = 0
    bad = 0
    total = 0
    detailed_results = []
    
    for i, test_input in enumerate(test_examples):
        print(f"\n--- Test {i+1} ---")
        print(f"INPUT: {test_input}")
        
        inputs = tokenizer(test_input, return_tensors="pt", max_length=config['max_length'], truncation=True)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=config['max_length'],
                num_beams=5,
                temperature=0.1,
                do_sample=False
            )
        
        result = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"OUTPUT: {result}")
        
        # Validate JSON
        is_valid_json = False
        try:
            parsed = json.loads(result)
            print(f"‚úÖ Valid JSON - Operation: {parsed.get('operacion', 'N/A')}")
            is_valid_json = True
        except:
            print(f"‚ùå Invalid JSON")
        
        expected_output = test_data[i]['output']  # Expected output
        print(f"EXPECTED: {expected_output}")
        
        # Exact comparison
        is_perfect = result.strip() == expected_output.strip()
        if is_perfect:
            print("‚úÖ PERFECT PREDICTION")
            perfect += 1
        else:
            print("‚ùå DIFFERENCES FOUND")
            bad += 1
        
        total += 1
        
        # Store detailed results
        detailed_results.append({
            'test_id': i + 1,
            'input': test_input,
            'expected': expected_output,
            'predicted': result,
            'is_perfect': is_perfect,
            'is_valid_json': is_valid_json
        })
    
    # Calculate metrics
    accuracy = (perfect / total) * 100 if total > 0 else 0
    valid_json_count = sum(1 for r in detailed_results if r['is_valid_json'])
    json_accuracy = (valid_json_count / total) * 100 if total > 0 else 0
    
    print(f"\nüìä {model_name} Performance Summary:")
    print(f"   ‚Ä¢ Perfect cases: {perfect}/{total} ({accuracy:.2f}%)")
    print(f"   ‚Ä¢ Failed cases: {bad}/{total} ({(bad/total)*100:.2f}%)")
    print(f"   ‚Ä¢ Valid JSON outputs: {valid_json_count}/{total} ({json_accuracy:.2f}%)")
    
    return {
        'model_name': model_name,
        'perfect': perfect,
        'bad': bad,
        'total': total,
        'accuracy': accuracy,
        'valid_json_count': valid_json_count,
        'json_accuracy': json_accuracy,
        'detailed_results': detailed_results
    }

# =============================================================================
# TEST BASE MODEL (BEFORE TRAINING) - COMMENTED OUT
# =============================================================================
# print("\nüî¨ TESTING BASE MODEL (BEFORE TRAINING)")
# print("=" * 80)

# # Create a copy of the base model for testing
# base_model = AutoModelForSeq2SeqLM.from_pretrained(CONFIG['model_name'])
# base_model = base_model.to(CONFIG['device'])

# base_results = test_model_performance(
#     base_model, test_data, tokenizer, CONFIG['device'], CONFIG, "BASE MODEL"
# )

# # Clear memory
# del base_model
# torch.cuda.empty_cache()

# PLACEHOLDER for base_results when not testing base model
base_results = {
    'model_name': 'BASE MODEL (SKIPPED)',
    'perfect': 0,
    'bad': 0, 
    'total': 0,
    'accuracy': 0.0,
    'valid_json_count': 0,
    'json_accuracy': 0.0,
    'detailed_results': []
}

# =============================================================================
# TRAINING PHASE
# =============================================================================
print("\nüöÄ STARTING FINE-TUNING ON A100")
print("=" * 60)

# Create directory for models
os.makedirs('./models', exist_ok=True)

# Training function
def train_epoch(model, dataloader, optimizer, device):
    model.train()
    total_loss = 0
    
    for batch_idx, batch in enumerate(dataloader):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        
        # Forward pass
        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels
        )
        
        loss = outputs.loss
        
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        
        print(f"   Batch {batch_idx+1}/{len(dataloader)}, Loss: {loss.item():.4f}")
    
    return total_loss / len(dataloader)

# Validation function
def validate(model, dataloader, device):
    model.eval()
    total_loss = 0
    
    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )
            
            total_loss += outputs.loss.item()
    
    return total_loss / len(dataloader)

# MAIN TRAINING LOOP
best_val_loss = float('inf')
training_history = []

for epoch in range(CONFIG['epochs']):
    print(f"\nüìà EPOCH {epoch + 1}/{CONFIG['epochs']}")
    print("-" * 40)
    
    # Train
    train_loss = train_epoch(model, train_loader, optimizer, CONFIG['device'])
    
    # Validate
    val_loss = validate(model, val_loader, CONFIG['device'])
    
    # Save best model
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), './models/best_robotics_model.pth')
        print(f"   üíæ Best model saved (val_loss: {val_loss:.4f})")
    
    # Log
    print(f"   üìä Train Loss: {train_loss:.4f}")
    print(f"   üìä Val Loss: {val_loss:.4f}")
    
    training_history.append({
        'epoch': epoch + 1,
        'train_loss': train_loss,
        'val_loss': val_loss
    })

# Final evaluation on test set
print(f"\nüß™ FINAL EVALUATION ON TEST SET")
test_loss = validate(model, test_loader, CONFIG['device'])
print(f"üìä Test Loss: {test_loss:.4f}")

# =============================================================================
# TEST TRAINED MODEL (AFTER TRAINING)
# =============================================================================
trained_results = test_model_performance(
    model, test_data, tokenizer, CONFIG['device'], CONFIG, "TRAINED MODEL"
)

# =============================================================================
# COMPARISON ANALYSIS
# =============================================================================
print("\nüî¨ DETAILED COMPARISON: BASE vs TRAINED MODEL")
print("=" * 80)

print(f"\nüìä OVERALL PERFORMANCE COMPARISON:")
print(f"   BASE MODEL:")
print(f"      ‚Ä¢ Accuracy: {base_results['accuracy']:.2f}% ({base_results['perfect']}/{base_results['total']})")
print(f"      ‚Ä¢ Valid JSON: {base_results['json_accuracy']:.2f}% ({base_results['valid_json_count']}/{base_results['total']})")
print(f"   TRAINED MODEL:")
print(f"      ‚Ä¢ Accuracy: {trained_results['accuracy']:.2f}% ({trained_results['perfect']}/{trained_results['total']})")
print(f"      ‚Ä¢ Valid JSON: {trained_results['json_accuracy']:.2f}% ({trained_results['valid_json_count']}/{trained_results['total']})")

# Calculate improvement
accuracy_improvement = trained_results['accuracy'] - base_results['accuracy']
json_improvement = trained_results['json_accuracy'] - base_results['json_accuracy']

print(f"\nüìà IMPROVEMENT:")
print(f"   ‚Ä¢ Accuracy improvement: {accuracy_improvement:+.2f} percentage points")
print(f"   ‚Ä¢ JSON validity improvement: {json_improvement:+.2f} percentage points")

# Detailed case-by-case comparison
print(f"\nüîç CASE-BY-CASE COMPARISON:")
print("-" * 80)

for i in range(len(test_data)):
    # base_case = base_results['detailed_results'][i]
    trained_case = trained_results['detailed_results'][i]
    
    # print(f"\nTest {i+1}: {base_case['input'][:50]}...")
    # print(f"   Base model:    {'‚úÖ' if base_case['is_perfect'] else '‚ùå'} Perfect | {'‚úÖ' if base_case['is_valid_json'] else '‚ùå'} JSON")
    print(f"   Trained model: {'‚úÖ' if trained_case['is_perfect'] else '‚ùå'} Perfect | {'‚úÖ' if trained_case['is_valid_json'] else '‚ùå'} JSON")
    
    # if base_case['is_perfect'] != trained_case['is_perfect']:
    #     if trained_case['is_perfect']:
    #         print(f"   üéâ IMPROVED: Base failed ‚Üí Trained succeeded")
    #     else:
    #         print(f"   ‚ö†Ô∏è DEGRADED: Base succeeded ‚Üí Trained failed")

# Save final model
timestamp = datetime.now().strftime("%Y%m%d_%H%M")
torch.save(model.state_dict(), f'./models/robotics_codet5_final_{timestamp}.pth')

print(f"\nüéâ FINE-TUNING COMPLETED!")
print(f"üíæ Models saved in ./models/")

# Show training summary
print(f"\nüìä TRAINING SUMMARY:")
for h in training_history:
    print(f"   Epoch {h['epoch']}: Train={h['train_loss']:.4f}, Val={h['val_loss']:.4f}")

# =============================================================================
# PERFORMANCE BY OPERATION TYPE
# =============================================================================
print(f"\nüìà PERFORMANCE BY OPERATION TYPE:")
print("-" * 50)

operation_stats = {}
for result in trained_results['detailed_results']:
    try:
        op = json.loads(result['expected']).get('operacion', 'unknown')
        if op not in operation_stats:
            operation_stats[op] = {'perfect': 0, 'total': 0}
        operation_stats[op]['total'] += 1
        if result['is_perfect']:
            operation_stats[op]['perfect'] += 1
    except:
        pass

for op, stats in sorted(operation_stats.items()):
    accuracy = (stats['perfect'] / stats['total']) * 100 if stats['total'] > 0 else 0
    print(f"   ‚Ä¢ {op}: {stats['perfect']}/{stats['total']} ({accuracy:.1f}%)")

# Save comparison results to file - MODIFIED FOR TRAINED MODEL ONLY
comparison_report = {
    'timestamp': timestamp,
    'config': CONFIG,
    # 'base_model_results': base_results,  # Commented out when base model not tested
    'trained_model_results': trained_results,
    'operation_performance': operation_stats,
    'training_history': training_history,
    # 'improvements': {  # Commented out when no base model comparison
    #     'accuracy': accuracy_improvement,
    #     'json_validity': json_improvement
    # }
}

with open(f'./models/comparison_report_{timestamp}.json', 'w') as f:
    json.dump(comparison_report, f, indent=2)

print(f"\nüìÑ Detailed comparison report saved as: comparison_report_{timestamp}.json")
print(f"\nüöÄ Model trained and ready to use!")

print(f"\nüöÄ INICIANDO DESCARGA SEGURA DE MODELOS")

# Descargar el mejor modelo
best_model_path = './models/best_robotics_model.pth'
if os.path.exists(best_model_path):
    safe_download_model(best_model_path, "BEST_MODEL")
else:
    print(f"‚ö†Ô∏è Modelo 'best' no encontrado en {best_model_path}")

# Descargar el modelo final con timestamp
final_model_path = f'./models/robotics_codet5_final_{timestamp}.pth'
if os.path.exists(final_model_path):
    safe_download_model(final_model_path, "FINAL_MODEL")
else:
    print(f"‚ö†Ô∏è Modelo final no encontrado en {final_model_path}")